<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG - Annisaa Fitri Nurfirdausi, Eleonora Mancini, Paolo Torroni">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Depression Detection, Deep Neural Networks, Multimodal Data">
  <!-- TODO: List all authors -->
  <meta name="author" content="Annisaa Fitri Nurfirdausi, Eleonora Mancini, Paolo Torroni">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  

  Open Graph / Facebook
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="DISI, University of Bologna, Italy">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">


<!-- Twitter -->
<!-- <meta name="twitter:card" content="summary_large_image"> -->
<!-- TODO: Replace with your lab/institution Twitter handle -->
<!-- <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE"> -->
<!-- TODO: Replace with first author's Twitter handle -->
<!-- <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE"> -->
<!-- TODO: Same as paper title above -->
<!-- <meta name="twitter:title" content="PAPER_TITLE"> -->
<!-- TODO: Same as description above -->
<!-- <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS"> -->
<!-- TODO: Same as social preview image above -->
<!-- <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png"> -->
<!-- <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview"> -->


  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG">
  <meta name="citation_author" content="Nurfirdausi, Annisaa Fitri">
  <meta name="citation_author" content="Mancini, Eleonora">
  <meta name="citation_author" content="Torroni, Paolo">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="Preprint">
 <!--  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">-->
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG - Annisaa Fitri Nurfirdausi, Eleonora Mancini, Paolo Torroni | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Preprint -->
<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG",
    "description": "A systematic exploration of multimodal depression detection using EEG, speech, and text, comparing feature representations, neural encoders, and fusion strategies with subject-independent splits.",
    "author": [
      {
        "@type": "Person",
        "name": "Annisaa Fitri Nurfirdausi"
      },
      {
        "@type": "Person",
        "name": "Eleonora Mancini"
      },
      {
        "@type": "Person",
        "name": "Paolo Torroni"
      }
    ],
    "dateCreated": "2025-09-12",
    "keywords": ["Depression Detection", "EEG", "Multimodal Learning", "Speech", "Text", "Machine Learning", "Vision Transformer", "DenseNet", "Hyperparameter Tuning"],
    "abstract": "Depression is a widespread mental health disorder, yet its automatic detection remains challenging. This work systematically compares feature representations and modeling strategies across EEG, speech, and text, evaluates unimodal and multimodal configurations, and analyzes fusion strategies. Subject-independent splits ensure reproducibility. Results show pretrained embeddings outperform handcrafted features, EEG enhances multimodal detection, and trimodal models achieve state-of-the-art performance.",
    "citation": "Annisaa Fitri Nurfirdausi, Eleonora Mancini, Paolo Torroni. 'TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG.' Preprint, 2025.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "url": "https://annisaa.github.io/tri-dep/",
      "@id": "https://annisaa.github.io/tri-dep/"
    }
  }
  </script>
  
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://www.researchgate.net/profile/Annisa-Nurfirdausi" target="_blank">Annisaa Fitri Nurfirdausi</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=1Qk3rogAAAAJ&hl=it" target="_blank">Eleonora Mancini</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=uOZZjwsAAAAJ&hl=en" target="_blank">Paolo Torroni</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block">DISI, University of Bologna, Italy<br>ICASSP 2026</span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>


                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/annisaafitrinn/mdnet" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Depression is a widespread mental health disorder, yet its automatic detection remains challenging. Prior work has explored unimodal and multimodal approaches, with multimodal systems showing promise by leveraging complementary signals. However, existing studies are limited in scope, lack systematic comparisons of features, and suffer from inconsistent evaluation protocols. We address these gaps by systematically exploring feature representations and modeling strategies across EEG, together with speech and text. We evaluate handcrafted features versus pretrained embeddings, assess the effectiveness of different neural encoders, compare unimodal, bimodal, and trimodal configurations, and analyze fusion strategies with attention to the complementary role of EEG. Consistent subject-independent splits ensure reproducible benchmarking. Our results show that the combination of EEG enhances multimodal detection, pretrained embeddings outperform handcrafted features, and carefully designed trimodal models achieve state-of-the-art performance. Our work serves as a robust benchmark for future research in multimodal depression detection.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract —>


<!-- Baseline Works -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Baseline Works</h2>
    <div class="content has-text-justified">
      <p>
        In this study, we implemented from scratch two multimodal baselines for comparison, both combining EEG and audio signals on the MODMA dataset. 
        Yousufi et al. (2024) used DenseNet-121 as a feature extractor, while Qayyum et al. (2023) applied Vision Transformer (ViT) models for generating EEG and audio spectrograms.
      </p>
    </div>

    <!-- Subsection 1 -->
    <h3 class="title is-4">Data Preparation for Baseline Works</h3>
    <div class="content has-text-justified">
      <p>
        EEG preprocessing follows Yousufi et al. (2024), including a 0.4–45 Hz FIR bandpass filter, a 50 Hz notch filter, and average referencing. 
        We selected 29 channels (FP2, FP1, Fz, F7, F3, F4, F8, FT7, FC3, FCz, FC4, FT8, C3, C4, T3, CP3, CPz, CP4, T4, TP7, P3, Pz, P4, TP8, T5, T6, O1, Oz, O2) for analysis. 
        Since Qayyum et al. (2023) provide limited details, we applied the same preprocessing pipeline to both baselines for consistency.
        For audio pre-processing, since both studies generate audio spectrograms directly from the original sampling rate of 44 kHz, we assume that no additional preprocessing was performed on the speech signals prior to spectrogram extraction.
      </p>
    </div>

    <!-- Subsection 2 -->
    <h3 class="title is-4">EEG and Speech Spectrogram Generation</h3>
    <div class="content has-text-justified">
      <p>
        Following Yousufi et al. (2024) and Qayyum et al. (2023), we generate STFT spectrograms for EEG and mel-spectrograms for audio using <code>librosa</code>. 
        We apply <code>n_fft=1024</code>, <code>hop_length=512</code>, and <code>n_mels=64</code> (for audio). 
        The resulting spectrograms are saved as <code>.png</code> images for use with 2D models such as DenseNet-121 and ViT.
      </p>
    </div>

    <!-- Subsection 3 -->
    <h3 class="title is-4">Data Splitting</h3>
    <div class="content has-text-justified">
      <p>
        We use stratified 5-fold cross-validation with subject-level splitting to prevent data leakage. 
        In each fold, 10% of the training data is set aside for validation (using a fixed random seed), 
        resulting in train, validation, and test splits that preserve class balance.
      </p>
    </div>
  </div>
</section>

<!-- Our Works -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Methodology</h2>
    <div class="content has-text-justified">
      <p></p>
    </div>

    <!-- Subsection 1 -->
    <h3 class="title is-4">EEG Preprocessing</h3>
    <div class="content has-text-justified">
      <p>
        EEG preprocessing follows two pipelines depending on the feature extraction method. 
      </p>
      <p>
        <b>Pipeline 1:</b> We select 29 depression-related EEG channels 
        <code>[FP2, FP1, Fz, F7, F3, F4, F8, FT7, FC3, FCz, FC4, FT8, C3, C4, T3, CP3, CPz, CP4, T4, TP7, P3, Pz, P4, TP8, T5, T6, O1, Oz, O2]</code>. 
        Signals are bandpass filtered (0.5–50 Hz), notch filtered at 50 Hz, re-referenced to the average electrode, and segmented into 10-second windows.
      </p>
      <p>
        <b>Pipeline 2:</b> We replicated the preprocessing steps of the CBraMod model for the MUMTAZ depression dataset. 
        Signals are resampled to 200 Hz, bandpass filtered between 0.3–75 Hz, and notch filtered at 50 Hz. 
        19 channels are selected 
        <code>[FP2, FP1, F7, F3, F4, F8, FCz, C3, C4, T3, CPz, T4, P3, Pz, P4, T5, T6, O1, O2]</code>. 
        The signals are segmented into 5-second windows (1000 samples each at 200 Hz). 
        Each recording is divided into multiple fixed-length windows, and each window is further split into 5 non-overlapping patches of 200 samples each.
      </p>
    </div>
  </div>
</section>


<!-- Experiments and Results -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Experiments and Results</h2>

    <!-- Subsection 1 -->
    <h3 class="title is-4">Data Splitting</h3>
    <div class="content has-text-justified">
      <p>
        We perform all experiments at the subject level using stratified 5-fold cross-validation to preserve class balance. The same subject-level splits are used across experiments, ensuring reproducibility, fair comparability, and robust results reported as mean and standard deviation across folds.
      </p>

      <!-- Image -->
      <figure class="image is-fullwidth">
        <img src="static/images/data-split.png" alt="Subject split configurations" loading="lazy">
        <figcaption class="has-text-centered">
          Subject split configurations
        </figcaption>
      </figure>
      
  </div>

  <!-- Subsection 2 -->
    <h3 class="title is-4">Baseline Results</h3>
    <div class="content has-text-justified">
      <p>
        Below, we present the results of the baseline models, along with the improvements obtained after hyperparameter tuning and the addition of the text modality.
      </p>
    <!-- Image -->
    <figure class="image is-fullwidth">
      <img src="static/images/hyperparameter-baseline.png" alt="Illustration of stratified 5-fold cross-validation" loading="lazy">
      <figcaption class="has-text-centered">
        Classification performance of Vision Transformer for different pooling strategies
      </figcaption>
    </figure>

    <!-- Image -->
    <figure class="image is-fullwidth">
      <img src="static/images/Vit-ori.png" alt="Illustration of stratified 5-fold cross-validation" loading="lazy">
      <figcaption class="has-text-centered">
        Classification performance of Vision Transformer for different pooling strategies
      </figcaption>
    </figure>

    <!-- Image -->
    <figure class="image is-fullwidth">
      <img src="static/images/Dense-ori.png" alt="Illustration of stratified 5-fold cross-validation" loading="lazy">
      <figcaption class="has-text-centered">
        Classification performance of DenseNet-121 for different pooling strategies
      </figcaption>
    </figure>

    <!-- Image -->
    <figure class="image is-fullwidth">
      <img src="static/images/ViT-tune.png" alt="Illustration of stratified 5-fold cross-validation" loading="lazy">
      <figcaption class="has-text-centered">
        Performance comparison of ViT before and after manual tuning.
      </figcaption>
    </figure>

    <!-- Image -->
    <figure class="image is-fullwidth">
      <img src="static/images/Dense-tune.png" alt="Illustration of stratified 5-fold cross-validation" loading="lazy">
      <figcaption class="has-text-centered">
        Performance comparison of DenseNet-121 before and after manual tuning.
      </figcaption>
    </figure>

    <!-- Image -->
    <figure class="image is-fullwidth">
      <img src="static/images/baseline-text.png" alt="Illustration of stratified 5-fold cross-validation" loading="lazy">
      <figcaption class="has-text-centered">
        erformance comparison of baseline works with and without text modality
      </figcaption>
    </figure>
  
    </div>

<!-- Subsection 3 -->
    <h3 class="title is-4">Preliminary Studies</h3>
    <div class="content has-text-justified">
      <p>
        The framework supports various combinations of feature types, deep encoders, and late-fusion strategies. To structure the investigation and ensure feasibility, we first performed unimodal experiments for each modality (EEG, speech, and text) to identify the most effective model for each stream. These best-performing unimodal models were then integrated into the final multimodal pipeline.
        The result of preliminary studies can be seen below:
      </p>
    
          <!-- Image -->
    <figure class="image is-fullwidth">
      <img src="static/images/eeg-res.png" alt="Illustration of stratified 5-fold cross-validation" loading="lazy">
      <figcaption class="has-text-centered">
        Performance Results on EEG Modality
      </figcaption>
    </figure>

        <!-- Image -->
        <figure class="image is-fullwidth">
          <img src="static/images/speech-res.png" alt="Illustration of stratified 5-fold cross-validation" loading="lazy">
          <figcaption class="has-text-centered">
            Performance Results on Speech Modality
          </figcaption>
        </figure>

          <!-- Image -->
    <figure class="image is-fullwidth">
      <img src="static/images/text-res.png" alt="Illustration of stratified 5-fold cross-validation" loading="lazy">
      <figcaption class="has-text-centered">
        Performance Results on Text Modality
      </figcaption>
    </figure>

        <!-- Image -->
        <figure class="image is-fullwidth">
          <img src="static/images/multimodal-res.png" alt="Illustration of stratified 5-fold cross-validation" loading="lazy">
          <figcaption class="has-text-centered">
            Performance Results on Multimodal Fusion Strategies
    modalit
          </figcaption>
        </figure>
    </div>

    <!-- Subsection 4 -->
    <h3 class="title is-4">Proposed Model</h3>
    <div class="content has-text-justified">
      <p>
        Based on our extensive experiments, we identified the best-performing models, and their overall architecture is illustrated below:
      </p>
    
          <!-- Image -->
    <figure class="image is-fullwidth">
      <img src="static/images/best_architecture.png" alt="Illustration of best performing architectures observed" loading="lazy">
      <figcaption class="has-text-centered">
        Best Architecture.
      </figcaption>
    </figure>
  </div>
</section>


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
